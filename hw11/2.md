using CairoMakie

# Define Booth function
booth(x, y) = (x + 2y - 7)^2 + (2x + y - 5)^2
booth_vec(v) = booth(v[1], v[2])

# Manual gradient computation
function booth_gradient(point)
    x, y = point[1], point[2]
    df_dx = 2*(x + 2y - 7) + 4*(2x + y - 5)
    df_dy = 4*(x + 2y - 7) + 2*(2x + y - 5)
    return [df_dx, df_dy]
end

# Gradient Descent
function gradient_descent_opt(func, grad_func, start_point; learning_rate=0.01, iterations=2000)
    current_point = copy(start_point)
    values = Float64[]
    
    for i in 1:iterations
        push!(values, func(current_point))
        gradient = grad_func(current_point)
        current_point = current_point - learning_rate * gradient
    end
    
    push!(values, func(current_point))
    return current_point, values
end

# Momentum
function momentum_opt(func, grad_func, start_point; learning_rate=0.01, momentum_coeff=0.9, iterations=2000)
    current_point = copy(start_point)
    velocity = zeros(size(current_point))
    values = Float64[]
    
    for i in 1:iterations
        push!(values, func(current_point))
        gradient = grad_func(current_point)
        velocity = momentum_coeff * velocity + learning_rate * gradient
        current_point = current_point - velocity
    end
    
    push!(values, func(current_point))
    return current_point, values
end

# Adam (simplified implementation)
function adam_opt(func, grad_func, start_point; learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, iterations=2000)
    current_point = copy(start_point)
    m = zeros(size(current_point))
    v = zeros(size(current_point))
    values = Float64[]
    
    for t in 1:iterations
        push!(values, func(current_point))
        gradient = grad_func(current_point)
        
        m = beta1 * m + (1 - beta1) * gradient
        v = beta2 * v + (1 - beta2) * gradient.^2
        
        m_hat = m / (1 - beta1^t)
        v_hat = v / (1 - beta2^t)
        
        current_point = current_point - learning_rate * m_hat / (sqrt.(v_hat) + epsilon)
    end
    
    push!(values, func(current_point))
    return current_point, values
end

# Initial point
initial = [-5.0, -5.0]

# Run optimizers
final_gd, gd_values = gradient_descent_opt(booth_vec, booth_gradient, initial, 
    learning_rate=0.01, iterations=2000)

final_momentum, momentum_values = momentum_opt(booth_vec, booth_gradient, initial,
    learning_rate=0.01, momentum_coeff=0.9, iterations=2000)

final_adam, adam_values = adam_opt(booth_vec, booth_gradient, initial,
    learning_rate=0.1, beta1=0.9, beta2=0.999, iterations=2000)

println("GD: Final point = [$(round(final_gd[1], digits=4)), $(round(final_gd[2], digits=4))], f = $(round(booth_vec(final_gd), digits=6))")
println("Momentum: Final point = [$(round(final_momentum[1], digits=4)), $(round(final_momentum[2], digits=4))], f = $(round(booth_vec(final_momentum), digits=6))")
println("Adam: Final point = [$(round(final_adam[1], digits=4)), $(round(final_adam[2], digits=4))], f = $(round(booth_vec(final_adam), digits=6))")

# Create comparison plot
figure = Figure(resolution=(900, 600))
axis = Axis(figure[1, 1],
    xlabel="Iteration",
    ylabel="log(Function Value)",
    title="Booth Function Optimization: GD vs Momentum vs Adam"
)

iterations = 0:2000
lines!(axis, iterations, log.(gd_values), label="Gradient Descent")
lines!(axis, iterations, log.(momentum_values), label="Momentum")
lines!(axis, iterations, log.(adam_values), label="Adam")

Legend(figure[1, 2], axis, "Methods")
figure