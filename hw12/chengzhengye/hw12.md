# HW11

## Q1
	•	Snippet B runs faster—approximately 1–2 orders of magnitude (≈20–100×) depending on hardware, because it transfers data only twice (once upload, once download) while Snippet A transfers every iteration.
	•	Anti-pattern in Snippet A: Repeated host↔GPU PCIe transfers inside the loop cause high latency and low bandwidth usage relative to GPU memory operations, dominating runtime.
	•	Golden rule violated: Minimize CPU–GPU data movement; maximize compute where data resides (GPU).
	•	Bandwidth estimate:
	•	PCIe 3.0 x16: ~12–16 GB/s
	•	GPU memory ops: ~300–1000 GB/s (≈20–60× higher)

  ## Q2
  1.
	•	Problem: The if i % 2 == 0 splits each warp so half the threads take the sin branch and half take the cos branch.
	•	On SIMT hardware, branches in a warp are executed serially, so the warp runs sin with half the lanes active, then cos with the other half.
	•	Result: In this simple two-way split, performance can be up to ~2× slower than if all threads executed the same path.

⸻

2. 
	•	SIMT = Single Instruction, Multiple Threads: all threads in a warp are supposed to execute the same instruction stream in lockstep, just on different data.
	•	Kernel A causes branch divergence inside a warp (even vs odd indices doing different work), so the warp no longer executes a single instruction stream. The hardware has to serialize the branches, breaking the SIMT ideal.

⸻

3. 
	•	Problem: B[i] = A[i * stride] means threads in a warp access A at strided, non-consecutive addresses.
	•	This leads to non-coalesced memory access: the hardware must issue many separate memory transactions instead of one or a few wide ones.
	•	Coalesced access: when thread 1, 2, 3, … in a warp read/write addresses that are consecutive (or nicely aligned), allowing the GPU to merge them into a small number of large, efficient memory transactions.

4. i = (blockIdx().x - 1) * blockDim().x + threadIdx().x


## Q3
1. 
	•	Approach B is faster.
	•	A does:
	•	y = x .^ 2 → 1 kernel
	•	z = sin.(y) → 1 kernel
	•	w = z .+ 1 → 1 kernel
→ 3 kernel launches + 2 intermediate arrays on GPU.
	•	B (@. sin(x^2) + 1) fuses the whole expression into 1 broadcast kernel, with no intermediate arrays.
→ Less launch overhead, less global memory traffic → faster.

⸻

2. 
	•	Kernel fusion: combining several elementwise operations into one GPU kernel so that each element is:
	1.	Loaded from memory once,
	2.	All operations applied in registers,
	3.	Written back once.
	•	Julia’s broadcasting (.@ or dotted syntax) analyzes the whole expression and generates a single fused kernel for compatible elementwise operations, giving fusion “for free” when you write vectorized code correctly.

⸻

3. 
	•	CUBLAS is NVIDIA’s highly optimized BLAS library:
	•	Uses blocked algorithms, shared memory, register tiling, SIMD, and often tensor cores.
	•	Tuned per-architecture by GPU experts.
	•	A naive custom kernel usually:
	•	Loads data inefficiently,
	•	Misses coalescing and tiling,
	•	Underutilizes compute units.
→ CUBLAS achieves near-peak GPU performance, whereas naive matmul can be orders of magnitude slower.

⸻

4. 
	•	(a) 2D image filtering → typically convolution / frequency-domain ops → CUFFT
	•	(b) Graph algorithms on sparse matrices → sparse linear algebra → CUSPARSE
	•	(c) Dense neural network training → dominated by dense GEMMs → CUBLAS