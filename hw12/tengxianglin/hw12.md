## Homework 12 — GPU Performance & CUDA.jl Analysis

All answers below assume a typical desktop GPU used with Julia + CUDA.jl (e.g., an NVIDIA card connected over PCIe). The numerical values are rough order-of-magnitude estimates; the focus is on **scaling behavior and principles**, not exact hardware specs.

---

### 1. Code Analysis: Performance Anti-patterns

We compare the following two snippets:

```julia
# Snippet A
x_cpu = randn(10000)
for i in 1:100
    x_gpu = CuArray(x_cpu)    # Upload
    x_gpu .+= 1
    x_cpu = Array(x_gpu)      # Download
end
```

```julia
# Snippet B
x_gpu = CuArray(randn(10000))  # Upload once
for i in 1:100
    x_gpu .+= 1                # All on GPU
end
result = Array(x_gpu)          # Download once
```

#### (1) Which code will run faster and by approximately how much?

- **Snippet B will be much faster.**  
- **Reason:**  
  - In Snippet A, every loop iteration does:  
    - `x_gpu = CuArray(x_cpu)`: one CPU→GPU transfer over PCIe (upload);  
    - `x_cpu = Array(x_gpu)`: one GPU→CPU transfer over PCIe (download).  
    So in total there are **100 uploads + 100 downloads**.  
  - In Snippet B, we upload once at the beginning and download once at the end, while all 100 updates are performed in-place on the GPU.  

Assume:
- Vector length is 10,000, element type `Float64`, so each element is 8 bytes and total size is about \(10000 \times 8 = 80\,\text{kB}\).  
- Even though a single transfer is small, PCIe transfers have **non-trivial fixed overheads** (DMA setup, synchronization, driver overhead), plus the cost of `CuArray` / `Array` construction. Repeating this **100 times** amplifies overhead dramatically.

Empirically, this kind of “upload–compute–download” pattern in a tight loop often makes Snippet A **10–100× slower** than Snippet B, and sometimes worse in real codes:
- Snippet B: one upload + one download + 100 GPU-internal memory operations and arithmetic (very fast, high bandwidth).  
- Snippet A: 100 rounds of PCIe transfers + allocations / frees / synchronizations, so GPU compute is mostly waiting for data.

#### (2) Identify the performance problem in Snippet A and explain why it occurs

- **Core performance issue:**  
  - **Excessive CPU↔GPU data transfers (host–device transfers).**  
  - Each small computation requires uploading data to the GPU and downloading it back, so overall performance is dominated by the **high-latency, lower-bandwidth PCIe bus** instead of by GPU arithmetic.  
- **Why is this slow?**  
  - GPU on-device memory bandwidth (reads/writes inside VRAM) is extremely high (hundreds of GB/s).  
  - PCIe bandwidth between CPU and GPU is much lower (tens of GB/s or less), and each transfer has fixed overhead.  
  - Frequent, small transfers accumulate those fixed costs and dominate runtime, while GPU’s parallel FLOPs stay underutilized.  
  - Additionally, each `CuArray(x_cpu)` / `Array(x_gpu)` may trigger memory allocations, garbage collection, and synchronization, adding further overhead.

#### (3) What is the "golden rule" that Snippet B follows but Snippet A violates?

- **Golden rule for GPU computing:**  
  - **Keep data on the GPU for as long as possible and minimize host–device transfers.**  
  - In other words: **upload once, do as much work as you can on the GPU, then download once.**
- Snippet B:  
  - Uploads `x_gpu` once, performs the 100 updates `x_gpu .+= 1` entirely on the device, and downloads only at the end → **follows the golden rule**.  
- Snippet A:  
  - Uploads and downloads in every iteration → **violates the “minimize transfers” principle**.

#### (4) Estimate the bandwidth difference between PCIe transfer and GPU memory operations

Typical orders of magnitude (consumer or workstation GPUs):  

- **GPU device memory bandwidth (within VRAM):**  
  - Roughly \(300\text{–}1000\,\text{GB/s}\) for modern GDDR6X / HBM devices (exact value depends on model).  
- **PCIe bandwidth (CPU↔GPU):**  
  - PCIe 3.0 x16 theoretical peak: about \(16\,\text{GB/s}\),  
  - PCIe 4.0 x16: about \(32\,\text{GB/s}\),  
  - Effective throughput is often lower in practice.  

Thus, **on-device GPU memory bandwidth is typically about one order of magnitude higher than PCIe bandwidth**:
- Around **10× or more** (e.g., \(500\,\text{GB/s}\) vs \(32\,\text{GB/s}\)).  
- Including latency and per-transfer overhead, the performance penalty of “many small PCIe transfers” is even worse than the raw bandwidth ratio suggests.

---

### 2. Code Analysis: Kernel Divergence and Memory Access

We consider these two kernels:

```julia
# Kernel A
function divergent_kernel(A)
    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x
    if i % 2 == 0
        A[i] = sin(A[i])    # Half of warp
    else
        A[i] = cos(A[i])    # Other half
    end
    return nothing
end
```

```julia
# Kernel B
function bad_memory_kernel(A, B, stride)
    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x
    @inbounds if i <= length(A)
        B[i] = A[i * stride]  # Non-sequential access!
    end
    return nothing
end
```

#### (1) What is wrong with Kernel A in terms of warp execution? How much could this slow down performance?

- **Issue:**  
  - For threads within the same warp, the condition `i % 2 == 0` alternates between `true` and `false`, so half of the threads execute the `sin` branch and the other half execute the `cos` branch.  
  - This causes **warp divergence**:  
    - When the warp executes the `if` branch, only the threads with even `i` are active, while the others are masked off;  
    - Then the warp executes the `else` branch with the opposite subset active.  
- **Consequence:**  
  - Effectively, the warp **serializes the two branches**, with only half of the threads doing useful work at any given time. This can reduce effective throughput by roughly **2×**.  
  - With more complex or nested branches, performance can degrade even further.

So due to severe intra-warp divergence, **Kernel A can easily be about 2× slower** than a branch-free (or branch-aligned) version, possibly worse in more complex cases.

#### (2) Explain what SIMT means and how Kernel A violates this execution model

- **SIMT (Single Instruction, Multiple Threads):**  
  - NVIDIA GPUs execute instructions in groups of threads called warps (typically 32 threads).  
  - **All threads in a warp execute the same instruction at the same time**, each with its own registers and data.  
  - The ideal is that every thread in the warp follows the same control flow (same instruction sequence), differing only in the data they operate on.  
- **Kernel A’s violation:**  
  - The branch condition `i % 2 == 0` sends different threads in the same warp down different control paths (`sin` vs `cos`).  
  - The hardware cannot execute two different instruction streams simultaneously for one warp, so it:  
    - Executes the `if` branch while masking out the threads that should take the `else` branch, then  
    - Executes the `else` branch while masking out the other threads.  
  - This breaks the SIMT ideal of “one instruction stream per warp” and turns it into a form of **serialized execution of divergent branches**.

#### (3) What memory access problem does Kernel B have? What is "coalesced" memory access?

- **Problem in Kernel B:**  
  - Thread `i` reads from `A[i * stride]`.  
  - When `stride > 1`, neighboring threads in a warp access indices `stride, 2*stride, 3*stride, ...` — i.e., they **skip elements in between**.  
  - This means the addresses accessed by adjacent threads in a warp are far apart and **not contiguous**.  
- **Consequence:**  
  - The GPU cannot combine these accesses into a small number of large memory transactions; instead it must issue many smaller, scattered memory operations.  
  - This leads to **poor effective memory bandwidth** and significantly slower kernels.

- **Coalesced memory access:**  
  - A memory access pattern where threads in a warp access **consecutive or near-consecutive addresses**, allowing the hardware to **coalesce** (merge) their requests into a few wide memory transactions.  
  - Coalesced access is key to getting close to peak global memory bandwidth.  
  - By contrast, **strided or irregular access** is **uncoalesced** and usually much slower.

#### (4) Rewrite the thread ID calculation to be more readable, explaining each component

Original:

```julia
i = (blockIdx().x - 1) * blockDim().x + threadIdx().x
```

More readable version with comments:

```julia
function bad_memory_kernel(A, B, stride)
    # blockIdx().x      – which block (along x) this is, 1-based
    # blockDim().x      – number of threads per block (along x)
    # threadIdx().x     – index of this thread within its block, 1-based

    # Map (blockIdx().x, threadIdx().x) to a global 1D thread index i
    global_block_id   = blockIdx().x - 1          # 0-based block index
    threads_per_block = blockDim().x              # threads in each block
    local_thread_id   = threadIdx().x             # 1-based index within block

    i = global_block_id * threads_per_block + local_thread_id

    @inbounds if i <= length(A)
        B[i] = A[i * stride]
    end
    return nothing
end
```

Explanation:
- **`global_block_id`**: which block this is along the x dimension, starting from 0 (convenient for arithmetic).  
- **`threads_per_block`**: how many threads are in each block.  
- **`local_thread_id`**: this thread’s index within its block (1-based).  
- **`i`**: the logical **global thread ID**, ranging from 1 up to the total number of launched threads, often used as an array index.

---

### 3. Code Analysis: Broadcasting vs Libraries

We compare:

```julia
# Approach A (multiple operations)
x = CUDA.randn(10000)
y = x .^ 2
z = sin.(y)
w = z .+ 1
```

```julia
# Approach B (fused operation)
x = CUDA.randn(10000)
w = @. sin(x^2) + 1
```

```julia
# Approach C (library function)
A = CUDA.randn(2000, 2000)
B = CUDA.randn(2000, 2000)
C = A * B  # Uses CUBLAS
```

#### (1) Which approach (A or B) will be faster and why? How many kernel launches does each require?

- **Kernel counts:**  
  - Approach A:  
    - `x .^ 2`: 1 kernel;  
    - `sin.(y)`: 1 kernel;  
    - `z .+ 1`: 1 kernel;  
    - Total: about **3 kernel launches**.  
  - Approach B:  
    - The `@.` macro fuses the whole expression `sin(x^2) + 1` into **a single broadcast kernel**, so there is **1 kernel launch**.  

- **Which is faster and why?**  
  - **Approach B is generally faster.** Reasons:  
    - Only 1 kernel launch, versus 3 in A (less launch overhead).  
    - B performs `x^2 → sin → +1` in a single kernel; for each element, it can load `x[i]` once, perform all arithmetic in registers, and store once.  
    - A must write and later reread intermediate arrays `y` and `z` from global memory, which adds extra global memory traffic.  
  - So B wins by **reducing kernel launch overhead and lowering global memory traffic**.

#### (2) What is "kernel fusion" and how does broadcasting achieve it automatically?

- **Kernel fusion:**  
  - Taking operations that would normally require several separate kernels and **merging them into one larger kernel** that does all the work in a single pass over the data.  
  - Benefits:  
    - Fewer kernel launches;  
    - Fewer writes/reads of intermediate results to/from global memory;  
    - Better cache and register utilization.  

- **How broadcasting implements fusion:**  
  - In Julia/CUDA.jl, broadcasted expressions like `@. sin(x^2) + 1` or `y .= f.(x) .+ g.(x)` are compiled into a **single fused GPU kernel**.  
  - That kernel, for each index `i`:  
    - Reads `x[i]`;  
    - Performs `x[i]^2`, then `sin`, then `+ 1` in registers;  
    - Writes the final result to output.  
  - There are no global arrays for `y[i]` or `z[i]` written and read in between, so the compiler effectively performs **automatic kernel fusion**.

#### (3) Why would Approach C (CUBLAS) be much faster than implementing matrix multiplication with custom kernels?

- **Reason:** CUBLAS is NVIDIA’s highly optimized BLAS library, designed specifically for dense linear algebra (including matrix–matrix multiplication). It uses many low-level optimizations, for example:  
  - Sophisticated **tiling and caching strategies**, loading tiles into shared memory and registers to reduce global memory traffic;  
  - Architecture-specific tuning for **SIMT execution, vectorization, and instruction-level parallelism**;  
  - Use of **Tensor Cores** and other special hardware units where available;  
  - Aggressive **loop unrolling, prefetching, and register allocation**;  
  - Support for many matrix shapes, batched GEMM, and multiple data types, each with tailored kernels.  

Compared to this, a hand-written “naive” GEMM kernel typically:
- Is just three nested loops with a simple thread mapping;  
- Does not use shared memory or caches efficiently;  
- Has suboptimal, non-coalesced memory access patterns;  
- Fails to fully exploit instruction pipelines and specialized hardware.  

As a result, **calling CUBLAS for matrix multiplication is usually orders of magnitude faster** than a naive custom kernel, and also far more reliable and portable.

#### (4) If you had to choose between CUBLAS, CUSPARSE, and CUFFT for the following problems, which would you pick?

(a) **2D image filtering**  
- Many 2D image filters (e.g., convolutions, frequency-domain filtering) can be implemented efficiently using the **FFT**: transform the image and kernel, multiply in the frequency domain, then inverse-transform.  
- Among the three options, **CUFFT** (Fast Fourier Transform) is the closest match for such workloads.  
- In practice, higher-level libraries (e.g., cuDNN, image processing libraries) often wrap these operations.

(b) **Graph algorithms on sparse matrices**  
- Graph adjacency matrices are typically very sparse, so sparse linear algebra is the right tool.  
- **CUSPARSE** is designed for sparse matrices and provides efficient kernels for sparse formats and operations.

(c) **Dense neural network training**  
- The core operation is large-scale dense matrix multiplication (GEMM).  
- **CUBLAS** (often via higher-level libraries like cuDNN or deep learning frameworks) is the right choice here.

**Summary of choices:**
- **2D image filtering** → primarily **CUFFT** (frequency-domain methods).  
- **Graph algorithms on sparse matrices** → **CUSPARSE**.  
- **Dense neural network training** → **CUBLAS** (and higher-level libraries that build on it).  


